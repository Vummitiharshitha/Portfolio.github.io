<!DOCTYPE html>
<html lang="en">
<head>
    <title>Literature page</title>
    <style>
        h1{
            color: #98EECC;
            font-size: 50px;
            font-family: 'Courier New', Courier, monospace;
            text-align: center;
        }
        h2{
            color: #435B66;
            font-size: 30px;
            font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif;
            text-align: justify;
        }
        pre{
            color: #85A389;
            font-size: 25px;
            font-family: 'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif;
            text-align: justify;
        }
        body{
            background-color: #FFF0F5;
        }
    </style>
</head>
<body>
    <h1>Literature </h1>
    <h2>1.Multi-Objective Based Spatial-Temporal Feature Representation Learning Robust to Expression Intensity Variations for Facial Expression Recognition</h2>
    <pre>According to the research paper by Dae Hoe Kim, Wissam J. Baddar, Jinhyeok Jang,   Yong Man Ro in 2017.
In this paper, they propose a new spatial-temporal feature representation learning for FER that is robust to expression intensity variations. The proposed method utilizes representative expression-states which can be specified in facial sequences regardless of the expression intensity. The characteristics of facial expressions are encoded in two parts in this paper. As the first part, spatial image characteristics of the representative expression-state frames are learned via a convolutional neural network. In the second part, temporal characteristics of the spatial feature representation in the first part are learned with a long short-term memory of the facial expression.
    </pre>
    <h2>2.Deep Structured Learning for Facial Action Unit Intensity Estimation </h2>
    <pre>According to the research paper by Robert Walecki, Ognjen (Oggi) Rudovic, Vladimir Pavlovic, Bjoern Schuller, Maja Pantic in 2017.
The goal of this paper is to model these structures and estimate complex feature representations simultaneously by combining conditional random field (CRF) encoded AU dependencies with deep learning. To this end, they propose a novel Copula CNN deep learning approach for modeling multivariate ordinal variables. Our model accounts for ordinal structure in output variables and their non-linear dependencies via copula functions modeled as cliques of a CRF. These are jointly optimized with deep CNN feature encoding layers using a newly introduced balanced batch iterative training algorithm.
    </pre>
    <h2>3.Facial Expression Recognition Using Enhanced Deep 3D Convolutional Neural Networks </h2>
    <pre>According to the research paper by Behzad Hasani, Mohammad H. Mahoor in 2017.
This paper proposes a 3D Convolutional Neural Network method for FER in videos. This new network architecture consists of 3D Inception-Reset layers followed by an LSTM unit that together extracts the spatial relations within facial images as well as the temporal relations between different frames in the video. Facial landmark points are also used as inputs to our network which emphasize on the importance of facial components rather than the facial regions that may not contribute significantly to generating facial expressions. Our proposed method is evaluated using four publicly available databases in subject-independent and cross-database tasks and outperforms state-of-the-art methods.
    </pre>
    <h2>4.Multimodal 2D+3D Facial Expression Recognition with Deep Fusion Convolutional Neural Network </h2>
    <pre>According to the research paper by Huibin Li, Jian Sun, Zongben Xu, Liming Chen in 2017.
This paper presents a novel and efficient deep fusion convolutional neural network (DF- CNN) for multimodal 2D+3D facial expression recognition (FER). DF-CNN comprises a feature extraction subnet, a feature fusion subnet, and a SoftMax layer. In particular, each textured three-dimensional (3D) face scan is represented as six types of 2D facial attribute maps (i.e., geometry map, three normal maps, curvature map, and texture map), all of which are jointly fed into DF-CNN for feature learning and fusion learning, resulting in a highly concentrated facial representation (32-dimensional). Expression prediction is performed by two ways: 1) learning linear support vector machine classifiers using the 32- dimensional fused deep features, or 2) directly performing SoftMax prediction using the six-dimensional expression probability vectors.
    </pre>
    <h2>5.Local Directional Ternary Pattern for Facial Expression Recognition</h2>
    <pre>According to the research paper by Byungyong Ryu, Adín Ramírez Rivera, Jaemyun Kim, Oksam Chae in 2017.
This paper presents a new face descriptor, local directional ternary pattern (LDTP), for facial expression recognition. LDTP efficiently encodes information of emotion-related features (i.e., eyes, eyebrows, upper nose, and mouth) by using the directional information and ternary pattern in order to take advantage of the robustness of edge patterns in the edge region while overcoming weaknesses of edge-based methods in smooth regions. They proposal, unlike existing histogram-based face description methods that divide the face into several regions and sample the codes uniformly, uses a two-level grid to construct the face descriptor while sampling expression-related information at different scales.
    </pre>
    <h2>6.Automatic facial expression recognition based on a deep convolutional-neural-network structure </h2>
    <pre>According to the research paper by Ke Shan, Junqi Guo, Wenwan You, Di Lu, Rongfang Bie in 2017.
In this paper, they employ a deep convolutional neural network (CNN) to devise a facial expression recognition system, which is capable to discover deeper feature representation of facial expression to achieve automatic recognition. The proposed system is composed of the Input Module, the Pre-processing Module, the Recognition Module and the Output Module. We introduce both the Japanese Female Facial Expression Database (JAFFE) and the Extended Cohn-Kanade Dataset (CK+) to simulate and evaluate the recognition performance under the influence of different factors (network structure, learning rate and pre-processing).
We also introduce a K-nearest neighbor (KNN) algorithm compared with CNN to make the results more convincing. The accuracy performance of the proposed system reaches 76.7442% and 80.303% in the JAFFE and CK+, respectively, which demonstrates feasibility and effectiveness of our system.
    </pre>
    <h2>7.A facial expression emotion recognition-based human-robot interaction system </h2>
    <pre>According to the research paper by Zhentao Liu, Min Wu, Weihua Cao,Luefeng Chen in 2017.
A facial emotion recognition method based on 2D-Gabor, uniform local binary pattern (LBP) operator, and multiclass extreme learning machine (ELM) classifier is presented, which is applied to real-time facial expression recognition for robots.
Facial expressions of robots are represented by simple cartoon symbols and displayed by a LED screen equipped in the robots, which can be easily understood by human. As a few prospective applications, the FEER-HRI system can be applied in home service, smart home, safe driving, and so on.
    </pre>
    <h2>8.Facial Expressions Recognition Based on Cognition nd Mapped Binary Patterns</h2>
    <pre> According to the research paper by Chao Qi, Min Li, Qiushi Wang, Huiquan Zhang in 2018.
In this paper, a new expression recognition approach is presented based on cognition and mapped binary patterns. At first, the approach is based on the LBP operator to extract the facial contours. Secondly, the establishment of pseudo-3-D model is used to segment the face area into six facial expression sub-regions. In this context, the sub-regions and the global facial expression images use the mapped LBP method for feature extraction, and then use two classifications which are the support vector machine and SoftMax with two kinds of emotion classification models the basic emotion model and the circumplex emotion model.
    </pre>
    <h2>9.A Convolutional Neural Network for Spotting Spontaneous Facial Micro-Expression from Long Videos </h2>
    <pre>According to the research paper by Zhihao Zhang, Tong Chen, Hongying Meng, Guangyuan Liu in 2018.
In this paper, we present a methodology for spotting micro-expression from long videos. Specifically, a new convolutional neural network named spotting micro-expression convolutional network was designed for extracting features from video clips, which is the first time that deep learning is used in micro-expression spotting. Then, a feature matrix processing method was proposed for spotting the apex frame from long video, which uses a sliding window and takes the characteristics of micro-expression into account to search the apex frame. Experimental results demonstrate that the proposed method can achieve a better performance than the existing state-of-art methods.
    </pre>
    <h2>10.Dominant and Complementary Emotion Recognition from Still Images of Faces </h2>
    <pre>According to the research paper by Jianzhu Guo, Zhen Lei, Jun Wan, Jun Wan in 2018.
In this paper, we analyze the top three winner methods and perform further detailed experiments on the proposed data set. Experiments indicate that pairs of compound emotion (e.g., surprisingly-happy vs happily-surprised) are more difficult to be recognized if compared with the seven basic emotions. However, we hope the proposed data set can help to pave the way for further research on compound facial emotion recognition.
    </pre>
</body>
</html>